{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Este tutorial é baseado no tutorial [Atenção é tudo que você precisa](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "O transformador descrito no artigo `Attention is All You Need` tem sido de grande interesse para vários pesquisadores em redes neurais. Além de produzir ganhos significativos em tarefas de tradução, este modelo provê uma nova arquitetura para muitas tarefas de processamento de linguagem natural (`NLP`).\n",
    "\n",
    "O artigo é extremamente bem escrito, mas é de conhecimento comum que esta arquitetura é bastante difícil de implementar de forma correta. Sendo assim, Alexander Rush escreveu esta versão linha-a-linha de como o modelo é implementado. Ele também teve ajuda de Vincent Nguyen e Guilherme Klein.\n",
    "\n",
    "De toda forma, eu espero que eu possa capturar de forma correta os argumentos/ideias descritas no artigo e explanadas pelos pesquisadores citados acima.\n",
    "\n",
    "### Dependências\n",
    "\n",
    "* torchtext\n",
    "* spacy\n",
    "* seaborn\n",
    "* torchtext\n",
    "* matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "\n",
    "Intra-atenção (`self-attention`) é um mecanismo de atenção que relaciona diferentes posições de uma única sequência de modo a computar a representação daquela sequência. Intra-antenção foi utilizado com sucesso em uma variedade de tarefas cujos nomes eu não me arrisco a traduzir (reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations).\n",
    "\n",
    "Redes neurais nesta arquitetura são baseadas em atenção recorrente ao invés de alinhamentos sequenciais e foi comprovado que esta arquitetura performa bem em tarefas de perguntas e respostas ou na representação de modelos de linguage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitetura\n",
    "\n",
    "Assim como a maioria dos modelos de sequência mais robustos, a arquiterura deste modelo segue a estrutura de um codificador-decodificador. \n",
    "\n",
    "Aqui, o codificador mapeia uma sequência de entrada com símbolos ($x_1, ..., x_n$) para uma sequência de representação continua z = ($z_1, ..., z_n$). \n",
    "\n",
    "Dado z, o decodificador gera então uma sequência de símbolos de saída ($y_1, ..., y_n$) - um símbolo a cada intervalo de tempo. A cada intervalo de tempo, o modelo é auto-regressivo, consumindo a última sequência de símbolos como parte da entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O transformador segue esta arquitetura empilhando uma série de camadas de intra-atenção, além de camadas densamente conectadas tanto no codificador quanto no decodificador, conforme exibidor no lado esquerdo e lado direito inferior da figura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/ModalNet-21.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mais detalhes explicando a figura em breve....`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilha de Codificadores e Decodificadores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificador\n",
    "\n",
    "O codificador é composto de uma pilha de $N = 6$ camadas idênticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos uma camada residual a cada duas camadas, e em seguida, utilizamos uma camada de normalização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em outras palavras, a saída de cada uma dessas camadas é uma $CamadaNormalizacao(x + SubCamada(x))$, onde $SubCamada(x)$ é a função implementada pela sub-camada. Também aplicamos dropout na saída de cada sub-camada antes de adicionarmos e normalizarmos os valores de entrada.\n",
    "\n",
    "Para facilitar a conexão entre estas camadas, todas as sub-camadas do modelo assim como as camadas intermediárias produzem saídas cujas dimensões são $d_{model} = 512$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada camada possui duas sub-camadas. A primeira, implementa um mecanismo de intra-atenção múltipla enquanto que a segunda camada implementa um algoritmo de feed-forward posicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decodificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O decodificador também é composto de uma pilha de $N = 6$ camadas idênticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em adição as duas sub-camadas existentes no codificador, o decodificador também possui uma terceira sub-camada, que computa atenção múltipla na saída da pilha de codificadores. De forma similar ao codificador, também utilizamos conexões residuais em cada subcamada, seguidas por uma camada de normalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é necessário que modifiquemos a camada de intra-atenção na pilha de decodificadores para previnir que uma determinada posição atenda à posições subsequentes. Esta modificação introduz um mecanismo de obfuscação (ou máscara) que combinado com o fato dos vetores estarem deslocados uma posição para a direita assegura que as predições na posição $i$ dependerão só e somente de saídas menores que $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Below the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atenção\n",
    "\n",
    "Uma função de atenção pode ser descrita como uma função que mapeia um consulta (`Q`) e um conjunto de chaves e valores (`K` e `V `) onde a consulta, chaves e valores assim como a saída da função são todos vetores. A saída é computada como uma soma ponderada dos valores, onde os pesos atribuídos a cada valor são computados através de uma função de compatibilidade entre a consulta e a sua correspondente chave.\n",
    "\n",
    "Cada mecanismo individual de atenção é chamado de **atenção de produto escalar** (`Scaled Dot-Product Attention`).\n",
    "Os dados de entrada consistem de uma consulta e chaves de dimensão $d_K$ e valores de dimensão $d_v$. Nos executamos o produto de cada termo da consulta com cada chave, dividindo cada um por $\\sqrt{d_K}$ e aplicando softmax para obter os pesos de cada valor computado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/attention.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**nota pessoal:** \n",
    "\n",
    "Há muita confusão quanto aos valores que `Q`, `K` e `V` podem assumir. No grupo de discussões de redes neurais, o aluno que explicou da seguinte forma:\n",
    "\n",
    "\n",
    "`TODO`\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prática, computamos a função de atenção em um conjunto de consultas simultaneamente. As consultas são agrupadas em uma matriz `Q` assim como as chaves e valores são agrupados em matrizes `K` e `V`. As matrizes de saída são computadas como:\n",
    "\n",
    "\\begin{equation}\n",
    "    Attention(Q, K, V) = softmax\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big)V\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k) # primeira caixa roxa (baixo para cima)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1) # caixa verde na figura\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn \n",
    "\n",
    "# são retornados dois valores, o primeiro é o resultado da função de anteção (multiplicação de matrizes)\n",
    "# o segundo é são os vetores de atenção, obtidos após o softmax.\n",
    "# TODO: indicar onde o vetor de atenção é utilizado em etapas posteriores do notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As duas funções mais comuns de atenção são atenção-aditiva e atenção-produto-escalar. A atenção por produto escalar é idêntica ao algoritmo acima exceto por um fator escalar (caixa amarela na imagem?) de $\\frac{1}{\\sqrt{d_k}}$. \n",
    "\n",
    "Apesar de teoricamente serem semelhantes, a atenção por produto escalar é mais rápida uma vez que ela pode se beneficiar de códigos que optimizam a multiplicação de matrizes. No entanto, há cenários onde a atenção aditiva é superior. Para uma discussão mais detalhada, direcionamos o leitor ao artigo original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
