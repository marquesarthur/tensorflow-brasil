{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Este tutorial é baseado no tutorial [Aprendendo representações de frases utilizando um codificador-decodificador para máquinas de tradução estatística](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb)\n",
    "\n",
    "### Dependências\n",
    "\n",
    "* torchtext\n",
    "* spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura de um codificador-decodificador\n",
    "\n",
    "Vamos lembrar da visão geral de um codificador-decodificador:\n",
    "\n",
    "![alt text](seq2seq1.png \"Title\")\n",
    "\n",
    "Nós usamos o codificador (em verde) na sequência de entrada para gerar um vetor de contexto `z` (em vermelho).\n",
    "Esse vetor é então utilizado em um decodificador (em azul) e uma camada linear (em roxo) para gerar a sequência de saída.\n",
    "\n",
    "Neste modelo, estamos usando um modelo de múltiplas camadas implementando uma memória de curto e longo prazo (`LSTM`):\n",
    "\n",
    "![alt text](seq2seq4.png \"Title\")\n",
    "\n",
    "Um dos problemas deste modelo, é que o decodificador está tentando colocar muita informação nos estados intermediários do nosso modelo.  No momento da decodificação, o estado intermediário deverá conter informação sobre toda a sequência de entrada codificada até o momento bem como todos os tokens decodificados até então. Isto exige muita memória. Seria interessante amenizar o processo de compressão para que possamos ter um modelo melhor!\n",
    "\n",
    "Para isso, usaremos uma unidade recorrente de porta (`GRU`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados\n",
    "\n",
    "Inicialmente, vamos importar algumas das bibliotecas necessárias para manipular os nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos usar a mesma `SEED` para garantir que os nossos resultados sejam reproduzíveis/determinísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, vamos utilizar modelos em alemão e inglês. \n",
    "\n",
    "**obs.:** Eu gostaria muito de que este tutorial fosse em alemão/português ou inglês/português, mas eu preciso achar um módulo/base de dados com ambos os idiomas\n",
    "\n",
    "No seu ambiente conda, execute a linha de comando abaixo para baixar os modelos:\n",
    "\n",
    "\n",
    "```bash\n",
    "python -m spacy download en\n",
    "python -m spacy download de\n",
    "```\n",
    "\n",
    "\n",
    "Em caso de sucesso, você obterá uma mensagem similar a:\n",
    "\n",
    "\n",
    "```bash\n",
    "    You can now load the model via spacy.load('en')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao processar os textos de entrada, vamos utilizar uma técnica chamada de [tokenização](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization). Basicamente, teremos uma sentença de entrada e fragmentaremos a sentence em unidades léxicas, como por exemplo, palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também vamos definir dois tokens especiais para indicar o início de uma sentença (`sos`) e o final de uma sentença (`eos`). \n",
    "Também converteremos a sentença para que ela contenha apenas letras minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos carregar nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos checar uma frase de entrada e saída para garantir que nossos dados estão corretos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'src': [   'zwei',\n",
      "               'junge',\n",
      "               'weiße',\n",
      "               'männer',\n",
      "               'sind',\n",
      "               'im',\n",
      "               'freien',\n",
      "               'in',\n",
      "               'der',\n",
      "               'nähe',\n",
      "               'vieler',\n",
      "               'büsche',\n",
      "               '.'],\n",
      "    'trg': [   'two',\n",
      "               'young',\n",
      "               ',',\n",
      "               'white',\n",
      "               'males',\n",
      "               'are',\n",
      "               'outside',\n",
      "               'near',\n",
      "               'many',\n",
      "               'bushes',\n",
      "               '.']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, vamos construir o nosso vocabulário convertendo todas as palavras que aparecem menos de duas vezes em termos desconhecidos (`<unk>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também vamos dividir os nossos dados em dados de treinamento, validação e testes. \n",
    "\n",
    "A primeira linha define se pytorch utilizará uma unidade de processamento `CUDA` ou uma `CPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo o nosso modelo\n",
    "\n",
    "### Codificador\n",
    "\n",
    "O codificador será um modelo com uma única camada de memória de curto longo prazo (`multi-layer LSTM`). E utilizaremos uma única unidade de porta recorrente. Não passaremos valores de dropout para a GRU uma vez que o dropout é utilizado entre as camadas de uma RNN. Como temos apenas uma única camada, PyTorch PyTorch alertará que temos uma única camada no nosso modelo.\n",
    "\n",
    "\n",
    "Outra informação importante sobre uma GRU é que ela requer e retorna uma única camada intermediária. Não há um estado como em uma LSTM.\n",
    "\n",
    "\\begin{equation}\n",
    "h_t = \\text{GRU}(x_t, h_{t - 1}) \\\\\n",
    "(h_t, c_t) = \\text{LSTM}(x_t, (h_{t - 1}, c_{t - 1})) \\\\\n",
    "h_t = \\text{RNN}(x_t, h_{t - 1})\n",
    "\\end{equation}\n",
    "\n",
    "De acordo com a equação acima, não há tanta diferença entre uma GRU e uma RNN. Entretanto, dentro de uma GRU existem inúmeros mecânismos de porta que controlam o fluxo de informação entrando e saindo de um estado intermediário. Para mais detalhes, checar este [post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (em inglês).\n",
    "\n",
    "**obs.:** No futuro, eu pretendo fazer um tutorial explicando em mais detalhes como uma LSTM funciona.\n",
    "\n",
    "\n",
    "O restante do codificador segue a arquitetura \"padrão\" de um codificador. Isto é, o codificador recebe a sequência de entrada $X = {x_1, x_2, ..., x_T}$ e computa estados intermediários de forma recorrende, $H = {h_1, h_2, ..., h_T}$. Por fim, é retornado o vetor de contexto (que corresponde ao último estado intermediário computado), $z = h_T$. Onde\n",
    "\n",
    "\\begin{equation}\n",
    "h_t = \\text{EncoderGRU}(x_t, h_{t - 1})\n",
    "\\end{equation}\n",
    "\n",
    "Este codificador é idêntico a arquitetura de um codificador de um modelo de sequência para sequência (`seq2seq`), mas toda a \"mágica\" ocorre dentro da GRU (em verde)\n",
    "\n",
    "![alt text](seq2seq5.png \"Title\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
    "        \n",
    "        #outputs = [src sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decodificador\n",
    "\n",
    "O decodificador é onde tentamos aliviar o problema da compressão de informação (discutido na introdução deste notebook).\n",
    "\n",
    "Ao invés da GRU no decodificador utilizar apenas as palavras de saída $y_t$ e o estado intermediário anterior $s_{t - 1}$ como entrada, o decodificador também utilizará o vetor de contexto $Z$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "s_t = \\text{DecoderGRU}(y_t, h_{t - 1}, Z)\n",
    "\\end{equation}\n",
    "\n",
    "Note que o vetor de contexto $Z$ não possui um parâmetro $t$. Iso indica que utilizamos o mesmo vetor de contexto (retornado pelo codificador) a cada intervalo de tempo $t$.\n",
    "\n",
    "Para predizermos o próximo estado, utilizaremos uma camada linear, $f$. Esta camada utiliza apenas a última camada intermediária do decodificador naquele intervalo de tempo, $s_t$. Sendo assim, para predizermos $\\hat{y}_{t + 1}$, passaremos como parâmetros o token atual na sequência $\\hat{y_t}$ e o vetor de contexto para uma camada linear, conforme equação:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}_{t + 1} = f(y_t, s_t, Z)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "O nosso decodificador se parece com algo como:\n",
    "\n",
    "\n",
    "![alt text](seq2seq6.png \"Title\") \n",
    "\n",
    "O estado intermediário inicial, $s_0$ é o vetor de contexto $Z$. Então, ao gerar o primeiro token da saída, nos estamos passando dois vetores indênticos para a GRU.\n",
    "\n",
    "**Como essas modificações reduzem a compressão de informação?** Tecnicamente, o estado intermediário $s_t$ não mais precisa manter qualquer informação sobre a sequência de entrada, uma vez que isto esta disponível como uma entrada através do vetor de contexto. Sendo assim, ele só precisa manter informação sobre os tokens gerados até então. A adição de $y_t$ a camanda linear também significa que esta camada pode utilizar qual é o último token visto, sem necessariamente necessitar desta informação como algo que deveria estar comprimido no último estado intermediário computado. \n",
    "\n",
    "Entretanto, a explicação anterior é apenas uma hipótese. É impossível determinar com precisão como o decodificador utiliza toda a informação fornecida. Dito isto, a explicação anterior serve como uma boa intuição para compreender o que está acontecendo e os resultados oriundos destas modificações indicam que estas modificações são uma boa ideia!\n",
    "\n",
    "**Implementação** passaremos $y_t$ e o vetor de contexto $Z$ para a GRU concatenando ambos os vetores. Desta forma, as dimensões do vetor de entrada da GRU serão *emb_dim + hid_dim*. A camada linear tomará como entrada $y_t$, $s_t$ e $Z$ concatenando estes vetores. Assim, o vetor de entrada para a camada linear terá dimensão *emb_dim + 2 x hid_dim*. Também não utilizaremos um dropout uma vez que a GRU só possui uma única camada.\n",
    "\n",
    "A função *forward* agora receberá um parâmetro, i.e. o contexto. Dentro da função, concatenaremos $y_t$ e $z$ em *emb_con* antes de passar este vetor para a GRU. Também concatenamos $y_t$, $s_t$ e $Z$ e passamos estes vetores para a camada linear para obtermos uma predição, isto é, $\\hat{y}_{t + 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(emb_dim + hid_dim*2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, context):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #context = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #context = [1, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0) # irá criar um vetor [...]\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        emb_con = torch.cat((embedded, context), dim=2)\n",
    "            \n",
    "        #emb_con = [1, batch size, emb dim + hid dim]\n",
    "            \n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1)\n",
    "        \n",
    "        #output = [batch size, emb dim + hid dim * 2]\n",
    "        \n",
    "        prediction = self.out(output)\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq \n",
    "\n",
    "Agora, vamos juntar todas as peças de nosso quebra-cabeças para gerar um modelo de sequência-para-sequência (`seq2seq`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is the context\n",
    "        context = self.encoder(src)\n",
    "        \n",
    "        #context also used as the initial hidden state of the decoder\n",
    "        hidden = context\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14,219,781 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model without a CUDA takes 6h+ no meu velho Macbook Pro 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 27m 4s\n",
      "\tTrain Loss: 4.633 | Train PPL: 102.818\n",
      "\t Val. Loss: 4.308 |  Val. PPL:  74.325\n",
      "Epoch: 02 | Time: 16m 59s\n",
      "\tTrain Loss: 3.617 | Train PPL:  37.217\n",
      "\t Val. Loss: 3.841 |  Val. PPL:  46.550\n",
      "Epoch: 03 | Time: 13m 53s\n",
      "\tTrain Loss: 3.173 | Train PPL:  23.870\n",
      "\t Val. Loss: 3.670 |  Val. PPL:  39.247\n",
      "Epoch: 04 | Time: 10m 33s\n",
      "\tTrain Loss: 2.868 | Train PPL:  17.603\n",
      "\t Val. Loss: 3.610 |  Val. PPL:  36.958\n",
      "Epoch: 05 | Time: 10m 30s\n",
      "\tTrain Loss: 2.630 | Train PPL:  13.877\n",
      "\t Val. Loss: 3.639 |  Val. PPL:  38.051\n",
      "Epoch: 06 | Time: 10m 38s\n",
      "\tTrain Loss: 2.439 | Train PPL:  11.463\n",
      "\t Val. Loss: 3.600 |  Val. PPL:  36.598\n",
      "Epoch: 07 | Time: 10m 33s\n",
      "\tTrain Loss: 2.304 | Train PPL:  10.017\n",
      "\t Val. Loss: 3.552 |  Val. PPL:  34.890\n",
      "Epoch: 08 | Time: 10m 32s\n",
      "\tTrain Loss: 2.176 | Train PPL:   8.812\n",
      "\t Val. Loss: 3.628 |  Val. PPL:  37.635\n",
      "Epoch: 09 | Time: 10m 36s\n",
      "\tTrain Loss: 2.084 | Train PPL:   8.033\n",
      "\t Val. Loss: 3.647 |  Val. PPL:  38.364\n",
      "Epoch: 10 | Time: 10m 29s\n",
      "\tTrain Loss: 2.000 | Train PPL:   7.389\n",
      "\t Val. Loss: 3.591 |  Val. PPL:  36.270\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'tut2_model.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.528 | Test PPL:  34.046 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
